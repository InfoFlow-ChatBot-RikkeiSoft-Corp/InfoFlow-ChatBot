{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chardet in c:\\users\\jhll0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.2.0)Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
            "[notice] To update, run: C:\\Users\\jhll0\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "pip install chardet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlN55ifShdgO"
      },
      "source": [
        "# Vector store-backed retriever\n",
        "\n",
        "벡터 스토어 리트리버는 벡터 스토어를 이용하여 문서를 검색하는 리트리버입니다. 리트리버 인터페이스에 부합하도록 벡터 스토어 클래스 주변의 가벼운 포장지입니다. 유사도 검색, MMR과 같이 벡터 스토어에서 구현된 검색 방법을 사용하여 벡터 스토어의 텍스트를 조회합니다.\n",
        "\n",
        "일단 벡터 스토어를 구축하면 리트리버를 구축하는 것은 매우 쉽습니다. 예를 들어 보겠습니다.\n",
        "\n",
        "Vector Store Retriever is a retriever that searches documents using the vector store. Lightweight wrapper around the vector store class to conform to the Retriever interface. Search text in a vector store using a search method implemented in the vector store, such as similarity search or MMR.\n",
        "\n",
        "Once you have built a vector store, building Retriever is very easy. Let me give you an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpuNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
            "[notice] To update, run: C:\\Users\\jhll0\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\jhll0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\jhll0\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp311-cp311-win_amd64.whl (13.8 MB)\n",
            "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/13.8 MB 330.3 kB/s eta 0:00:42\n",
            "   ---------------------------------------- 0.1/13.8 MB 660.6 kB/s eta 0:00:21\n",
            "   ---------------------------------------- 0.1/13.8 MB 939.4 kB/s eta 0:00:15\n",
            "    --------------------------------------- 0.3/13.8 MB 1.6 MB/s eta 0:00:09\n",
            "   - -------------------------------------- 0.6/13.8 MB 2.7 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 1.2/13.8 MB 4.5 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 2.1/13.8 MB 6.6 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 2.7/13.8 MB 7.6 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 3.6/13.8 MB 8.9 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 4.4/13.8 MB 9.6 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 4.9/13.8 MB 10.0 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 4.9/13.8 MB 10.0 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 4.9/13.8 MB 10.0 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 4.9/13.8 MB 8.0 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 4.9/13.8 MB 8.0 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 4.9/13.8 MB 6.8 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 5.0/13.8 MB 6.5 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 5.0/13.8 MB 6.3 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 5.1/13.8 MB 5.8 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 5.1/13.8 MB 5.7 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 5.2/13.8 MB 5.5 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 5.3/13.8 MB 5.3 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 5.4/13.8 MB 5.2 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 5.5/13.8 MB 5.0 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 5.7/13.8 MB 5.0 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 5.8/13.8 MB 4.9 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 6.0/13.8 MB 4.8 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 6.2/13.8 MB 4.8 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 6.3/13.8 MB 4.7 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 7.1/13.8 MB 5.1 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 7.6/13.8 MB 5.4 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 8.2/13.8 MB 5.6 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 8.9/13.8 MB 5.9 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 9.4/13.8 MB 6.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 10.1/13.8 MB 6.3 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 10.7/13.8 MB 7.0 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 11.4/13.8 MB 7.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 11.8/13.8 MB 7.0 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 11.8/13.8 MB 6.8 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 12.1/13.8 MB 6.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 12.5/13.8 MB 6.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 12.9/13.8 MB 6.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 13.2/13.8 MB 6.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.8/13.8 MB 6.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.8/13.8 MB 6.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 13.8/13.8 MB 6.1 MB/s eta 0:00:00\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ],
      "source": [
        "pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# OpenAIEmbeddings 초기화\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_SlZK05jhV7I"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"../../state_of_the_union.txt\")\n",
        "loader = TextLoader(r\"C:\\Users\\jhll0\\GitHub\\history-teller\\langchain document\\state_of_the_union.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File exists!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = r\"C:\\Users\\jhll0\\GitHub\\history-teller\\langchain document\\state_of_the_union.txt\"\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File exists!\")\n",
        "else:\n",
        "    print(\"File not found. Check the path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOM removed and file saved as UTF-8 without BOM.\n"
          ]
        }
      ],
      "source": [
        "# BOM 제거\n",
        "input_path = r\"C:\\Users\\jhll0\\GitHub\\history-teller\\langchain document\\state_of_the_union.txt\"\n",
        "output_path = r\"C:\\Users\\jhll0\\GitHub\\history-teller\\langchain document\\state_of_the_union_cleaned.txt\"\n",
        "\n",
        "with open(input_path, \"r\", encoding=\"utf-8-sig\") as f_in:  # utf-8-sig 처리\n",
        "    content = f_in.read()\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "    f_out.write(content)\n",
        "\n",
        "print(\"BOM removed and file saved as UTF-8 without BOM.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS database created successfully!\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# 텍스트 로드\n",
        "loader = TextLoader(\n",
        "    r\"C:\\Users\\jhll0\\GitHub\\history-teller\\langchain document\\state_of_the_union_cleaned.txt\",\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "documents = loader.load()\n",
        "\n",
        "# 텍스트 분할\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# 임베딩 및 FAISS DB 생성\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "print(\"FAISS database created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DSBWKCXghxvr"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever() #검색기 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xc2WPlYDiEkv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jhll0\\AppData\\Local\\Temp\\ipykernel_16948\\3496095554.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") #유사문장 검색\n"
          ]
        }
      ],
      "source": [
        "docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") #유사문장 검색"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsY47IvIiHbH"
      },
      "source": [
        "## Maximum marginal relevance retrieval\n",
        "\n",
        "벡터 스토어 리트리버는 기본적으로 유사성 검색을 사용합니다. 기본 벡터 스토어가 최대 한계 관련성 검색을 지원하는 경우 검색 유형으로 지정할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0tTxPUsmibgK"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_type=\"mmr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "B6Ml6iOeic2e"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") #위에꺼와 동일"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Taiwrer0nTdr"
      },
      "source": [
        "유사도 점수 임계값을 설정하고 해당 임계값 이상의 점수를 가진 문서만 반환하는 검색 방법을 설정할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "t1hP-FOwnShE"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PpHp5yZ4nkSj"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdYXckamnoed"
      },
      "source": [
        "## Specifying top k\n",
        "\n",
        "검색할 때 사용할 k와 같은 검색 kwarg를 지정할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Nl8WGLuSnyAO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\")\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s3u1D7on1zJ"
      },
      "source": [
        "# MultiQueryRetriever\n",
        "\n",
        "거리 기반 벡터 데이터베이스 검색은 고차원 공간에 쿼리를 내장(대표)하고 \"거리\"를 기반으로 유사한 내장 문서를 찾습니다. 그러나 검색은 쿼리 문구의 미묘한 변경으로 인해 또는 임베딩이 데이터의 의미론을 잘 포착하지 못하는 경우 다른 결과를 생성할 수 있습니다. 이러한 문제를 수동으로 해결하기 위해 신속한 엔지니어링/튜닝이 수행되기도 하지만 지루할 수 있습니다.\n",
        "\n",
        "MultiQueryRetriver는 LLM을 사용하여 특정 사용자 입력 쿼리에 대해 다양한 관점에서 여러 쿼리를 생성함으로써 프롬프트 튜닝 프로세스를 자동화합니다. 각 쿼리에 대해 관련 문서 집합을 검색하고 모든 쿼리에서 고유한 결합을 사용하여 잠재적으로 관련된 문서 집합을 더 많이 가져옵니다. MultiQueryRetriver는 동일한 질문에 대해 여러 관점을 생성함으로써 거리 기반 검색의 한계를 극복하고 보다 풍부한 결과 집합을 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a sample vectorDB\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Load blog post\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(data)\n",
        "\n",
        "# VectorDB\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J__nhAUoG5b"
      },
      "source": [
        "Simple usage\n",
        "\n",
        "Specify the LLM to use for query generation, and the retriever will do the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSvL2jVOoJPW"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "question = \"What are the approaches to Task Decomposition?\"\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5EjEKdKoN-L"
      },
      "outputs": [],
      "source": [
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmgBk-2LoPvb"
      },
      "outputs": [],
      "source": [
        "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
        "len(unique_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrEk6yexpJw6"
      },
      "source": [
        "Supplying your own prompt\n",
        "You can also supply a prompt along with an output parser to split the results into a list of queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxAuVk93oRL2"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Output parser will split the LLM result into a list of queries\n",
        "class LineList(BaseModel):\n",
        "    # \"lines\" is the key (attribute name) of the parsed output\n",
        "    lines: List[str] = Field(description=\"Lines of text\")\n",
        "\n",
        "\n",
        "class LineListOutputParser(PydanticOutputParser):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__(pydantic_object=LineList)\n",
        "\n",
        "    def parse(self, text: str) -> LineList:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return LineList(lines=lines)\n",
        "\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from a vector\n",
        "    database. By generating multiple perspectives on the user question, your goal is to help\n",
        "    the user overcome some of the limitations of the distance-based similarity search.\n",
        "    Provide these alternative questions separated by newlines.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Chain\n",
        "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
        "\n",
        "# Other inputs\n",
        "question = \"What are the approaches to Task Decomposition?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbPJsX3rpOpL"
      },
      "outputs": [],
      "source": [
        "# Run\n",
        "retriever = MultiQueryRetriever(\n",
        "    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
        ")  # \"lines\" is the key (attribute name) of the parsed output\n",
        "\n",
        "# Results\n",
        "unique_docs = retriever.get_relevant_documents(\n",
        "    query=\"What does the course say about regression?\"\n",
        ")\n",
        "len(unique_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnaXPav2pr8r"
      },
      "source": [
        "# Contextual compression\n",
        "One challenge with retrieval is that usually you don’t know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
        "\n",
        "Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
        "\n",
        "To use the Contextual Compression Retriever, you’ll need: - a base retriever - a Document Compressor\n",
        "\n",
        "The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether.\n",
        "\n",
        "검색 시 한 가지 문제는 일반적으로 데이터를 시스템에 가져올 때 문서 저장 시스템이 직면할 특정 쿼리를 모른다는 것입니다. 이는 쿼리와 가장 관련이 있는 정보가 관련 없는 텍스트가 많은 문서에 묻혀 있을 수 있음을 의미합니다. 해당 전체 문서를 응용 프로그램을 통해 전달하면 더 비싼 LLM 호출과 더 나쁜 응답으로 이어질 수 있습니다.\n",
        "\n",
        "이를 해결하기 위해서는 문맥 압축을 사용해야 합니다. 검색한 문서를 그대로 즉시 반환하는 대신 해당 쿼리의 컨텍스트를 사용하여 문서를 압축할 수 있으므로 관련 정보만 반환됩니다. 여기서 \"압축\"이란 개별 문서의 내용을 압축하는 것과 문서를 일괄적으로 필터링하는 것을 말합니다.\n",
        "\n",
        "Contextual Compression Retriever를 사용하려면 다음이 필요합니다. - Base Retriever - Document Compressor\n",
        "\n",
        "Contextual Compression Retriver는 기본 리트리버에 쿼리를 전달하고 초기 문서를 가져와 Document Compressor를 통과합니다. Document Compressor는 문서 목록을 가져가서 문서 내용을 줄이거나 문서를 모두 삭제하여 단축합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK0wy-DJqFmb"
      },
      "source": [
        "### Get started\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_aVvFetp90c"
      },
      "outputs": [],
      "source": [
        "# Helper function for printing docs\n",
        "\n",
        "\n",
        "def pretty_print_docs(docs):\n",
        "    print(\n",
        "        f\"\\n{'-' * 100}\\n\".join(\n",
        "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFh4X9DjqRUH"
      },
      "source": [
        "### Using a vanilla vector store retriever\n",
        "\n",
        "Let’s start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can see that given an example question our retriever returns one or two relevant docs and a few irrelevant docs. And even the relevant docs have a lot of irrelevant information in them.\n",
        "\n",
        "먼저 간단한 벡터 스토어 리트리버를 초기화하고 2023년 국정연설(청크 단위)을 저장하는 것으로 시작하겠습니다. 예제 질문이 주어지면 리트리버는 관련 문서를 한 두 개, 관련 없는 문서를 몇 개 반환한다는 것을 알 수 있습니다. 그리고 관련 문서에도 관련 없는 정보가 많이 포함되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRlF7vi-prn3"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "documents = TextLoader(\"../../state_of_the_union.txt\").load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
        "\n",
        "docs = retriever.get_relevant_documents(\n",
        "    \"What did the president say about Ketanji Brown Jackson\"\n",
        ")\n",
        "pretty_print_docs(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOfv1vo8q7ju"
      },
      "source": [
        "### Adding contextual compression with an LLMChainExtractor\n",
        "\n",
        "Now let’s wrap our base retriever with a ContextualCompressionRetriever. We’ll add an LLMChainExtractor, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.\n",
        "\n",
        "이제 기본 리트리버를 Contextual CompressionRetriver로 마무리하겠습니다. LLMChainExtractor를 추가하여 처음에 반환된 문서를 반복하고 쿼리와 관련된 내용만 추출합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQhcppdoq5tw"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\n",
        "    \"What did the president say about Ketanji Jackson Brown\"\n",
        ")\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVxdDULSrQox"
      },
      "source": [
        "### More built-in compressors: filters\n",
        "\n",
        "#### LLMChainFilter\n",
        "The LLMChainFilter is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents.\n",
        "\n",
        "LLMChainFilter는 LLM 체인을 사용하여 문서 내용을 조작하지 않고 처음 검색된 문서 중 필터링할 문서와 반환할 문서를 결정하는 보다 단순하지만 강력한 압축기입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "doAcXwR-rWZE"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.document_compressors import LLMChainFilter\n",
        "\n",
        "_filter = LLMChainFilter.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter, base_retriever=retriever\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\n",
        "    \"What did the president say about Ketanji Jackson Brown\"\n",
        ")\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyLzs5SroD2"
      },
      "source": [
        "### EmbeddingsFilter\n",
        "Making an extra LLM call over each retrieved document is expensive and slow. The EmbeddingsFilter provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.\n",
        "\n",
        "검색된 각 문서를 통해 추가 LLM 호출을 수행하는 것은 비용이 많이 들고 속도도 느립니다. 임베딩 필터는 문서와 쿼리를 임베딩하고 임베딩이 충분히 유사한 문서만 쿼리에 반환함으로써 더 저렴하고 빠른 옵션을 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7SC0Zk4rjlo"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=embeddings_filter, base_retriever=retriever\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\n",
        "    \"What did the president say about Ketanji Jackson Brown\"\n",
        ")\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6vjHoWAr42k"
      },
      "source": [
        "### Stringing compressors and document transformers together\n",
        "\n",
        "Using the DocumentCompressorPipeline we can also easily combine multiple compressors in sequence. Along with compressors we can add BaseDocumentTransformers to our pipeline, which don’t perform any contextual compression but simply perform some transformation on a set of documents. For example TextSplitters can be used as document transformers to split documents into smaller pieces, and the EmbeddingsRedundantFilter can be used to filter out redundant documents based on embedding similarity between documents.\n",
        "\n",
        "Below we create a compressor pipeline by first splitting our docs into smaller chunks, then removing redundant documents, and then filtering based on relevance to the query.\n",
        "\n",
        "또한 DocumentCompressor 파이프라인을 사용하여 여러 압축기를 순차적으로 쉽게 결합할 수 있습니다. 압축기와 함께 BaseDocumentTransformer를 파이프라인에 추가할 수 있으며, 이는 상황에 맞는 압축을 수행하지 않고 문서 집합에 대한 변환만 수행합니다. 예를 들어 TextSplitter는 문서를 더 작은 조각으로 분할하는 문서 변환기로 사용할 수 있으며, EmbeddingsRedundantFilter는 문서 간의 임베딩 유사성을 기반으로 중복 문서를 필터링하는 데 사용할 수 있습니다.\n",
        "\n",
        "아래에서는 먼저 문서를 더 작은 청크로 분할한 다음 중복 문서를 제거하고 쿼리와의 관련성을 기반으로 필터링하여 압축기 파이프라인을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loyWlGqssEt9"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
        "\n",
        "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
        "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
        "pipeline_compressor = DocumentCompressorPipeline(\n",
        "    transformers=[splitter, redundant_filter, relevant_filter]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyHucHqssFnR"
      },
      "outputs": [],
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=pipeline_compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\n",
        "    \"What did the president say about Ketanji Jackson Brown\"\n",
        ")\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGduOkH3sK-Z"
      },
      "source": [
        "# Ensemble Retriever\n",
        "The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and rerank the results based on the Reciprocal Rank Fusion algorithm.\n",
        "\n",
        "By leveraging the strengths of different algorithms, the EnsembleRetriever can achieve better performance than any single algorithm.\n",
        "\n",
        "The most common pattern is to combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity), because their strengths are complementary. It is also known as “hybrid search”. The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity.\n",
        "\n",
        "앙상블리트리버는 리트리버 목록을 입력으로 받아 get_related_documents() 메서드의 결과를 앙상블하고 상호 순위 퓨전 알고리즘을 기반으로 결과의 순위를 다시 매깁니다.\n",
        "\n",
        "다양한 알고리즘의 장점을 활용하여 앙상블 리트리버는 어떤 단일 알고리즘보다 더 나은 성능을 얻을 수 있습니다.\n",
        "\n",
        "가장 일반적인 패턴은 희소 리트리버(BM25와 같은)와 밀집 리트리버(임베딩 유사성과 같은)를 결합하는 것인데, 이는 장점이 상호 보완적이기 때문입니다. 이는 \"하이브리드 검색\"이라고도 합니다. 희소 리트리버는 키워드를 기반으로 관련 문서를 찾는 데 능숙하고, 밀집 리트리버는 의미 유사성을 기반으로 관련 문서를 찾는 데 능숙합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6GpLCGmsa6b"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  rank_bm25 > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0QyUGhjsIie"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SHcijy4seAz"
      },
      "outputs": [],
      "source": [
        "doc_list_1 = [\n",
        "    \"I like apples\",\n",
        "    \"I like oranges\",\n",
        "    \"Apples and oranges are fruits\",\n",
        "]\n",
        "\n",
        "# initialize the bm25 retriever and faiss retriever\n",
        "bm25_retriever = BM25Retriever.from_texts(\n",
        "    doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
        ")\n",
        "bm25_retriever.k = 2\n",
        "\n",
        "doc_list_2 = [\n",
        "    \"You like apples\",\n",
        "    \"You like oranges\",\n",
        "]\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "faiss_vectorstore = FAISS.from_texts(\n",
        "    doc_list_2, embedding, metadatas=[{\"source\": 2}] * len(doc_list_2)\n",
        ")\n",
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# initialize the ensemble retriever\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khSUmWJBshhV"
      },
      "outputs": [],
      "source": [
        "docs = ensemble_retriever.invoke(\"apples\")\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhgbXbZksjc5"
      },
      "source": [
        "## Runtime Configuration\n",
        "\n",
        "런타임에 검색기를 구성할 수도 있습니다. 이를 위해서는 필드를 구성 가능한 것으로 표시해야 합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alngqOmzsiN0"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "faiss_retriever = faiss_vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": 2}\n",
        ").configurable_fields(\n",
        "    search_kwargs=ConfigurableField(\n",
        "        id=\"search_kwargs_faiss\",\n",
        "        name=\"Search Kwargs\",\n",
        "        description=\"The search kwargs to use\",\n",
        "    )\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPMNQCTwswtR"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SF0P93ysyYp"
      },
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"search_kwargs_faiss\": {\"k\": 1}}}\n",
        "docs = ensemble_retriever.invoke(\"apples\", config=config)\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4TiCJ09s1Pq"
      },
      "source": [
        "# Long-Context Reorder\n",
        "\n",
        "No matter the architecture of your model, there is a substantial performance degradation when you include 10+ retrieved documents. In brief: When models must access relevant information in the middle of long contexts, they tend to ignore the provided documents. See: https://arxiv.org/abs/2307.03172\n",
        "\n",
        "To avoid this issue you can re-order documents after retrieval to avoid performance degradation.\n",
        "\n",
        "모델의 아키텍처에 상관없이 10개 이상의 검색된 문서를 포함할 경우 성능이 크게 저하됩니다. 간단히 말해서 모델이 긴 컨텍스트 중간에 관련 정보에 액세스해야 할 경우 제공된 문서를 무시하는 경향이 있습니다. https://arxiv.org/abs/2307.03172 참조\n",
        "\n",
        "이 문제를 방지하려면 검색 후 문서를 다시 정렬하여 성능 저하를 방지할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4kvuCpPtGJ7"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  sentence-transformers > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP1pCVq8tHUa"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain, StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_transformers import (\n",
        "    LongContextReorder,\n",
        ")\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Get embeddings.\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = [\n",
        "    \"Basquetball is a great sport.\",\n",
        "    \"Fly me to the moon is one of my favourite songs.\",\n",
        "    \"The Celtics are my favourite team.\",\n",
        "    \"This is a document about the Boston Celtics\",\n",
        "    \"I simply love going to the movies\",\n",
        "    \"The Boston Celtics won the game by 20 points\",\n",
        "    \"This is just a random text.\",\n",
        "    \"Elden Ring is one of the best games in the last 15 years.\",\n",
        "    \"L. Kornet is one of the best Celtics players.\",\n",
        "    \"Larry Bird was an iconic NBA player.\",\n",
        "]\n",
        "\n",
        "# Create a retriever\n",
        "retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
        "    search_kwargs={\"k\": 10}\n",
        ")\n",
        "query = \"What can you tell me about the Celtics?\"\n",
        "\n",
        "# Get relevant documents ordered by relevance score\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG17Zjb7tK8Q"
      },
      "source": [
        "```\n",
        "[Document(page_content='This is a document about the Boston Celtics'),\n",
        " Document(page_content='The Celtics are my favourite team.'),\n",
        " Document(page_content='L. Kornet is one of the best Celtics players.'),\n",
        " Document(page_content='The Boston Celtics won the game by 20 points'),\n",
        " Document(page_content='Larry Bird was an iconic NBA player.'),\n",
        " Document(page_content='Elden Ring is one of the best games in the last 15 years.'),\n",
        " Document(page_content='Basquetball is a great sport.'),\n",
        " Document(page_content='I simply love going to the movies'),\n",
        " Document(page_content='Fly me to the moon is one of my favourite songs.'),\n",
        " Document(page_content='This is just a random text.')]\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCnFcJH3tNiw"
      },
      "outputs": [],
      "source": [
        "# Reorder the documents:\n",
        "# Less relevant document will be at the middle of the list and more\n",
        "# relevant elements at beginning / end.\n",
        "reordering = LongContextReorder()\n",
        "reordered_docs = reordering.transform_documents(docs)\n",
        "\n",
        "# Confirm that the 4 relevant documents are at beginning and end.\n",
        "reordered_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpfvZ3ortQ7A"
      },
      "source": [
        "```\n",
        " [Document(page_content='The Celtics are my favourite team.'),\n",
        " Document(page_content='The Boston Celtics won the game by 20 points'),\n",
        " Document(page_content='Elden Ring is one of the best games in the last 15 years.'),\n",
        " Document(page_content='I simply love going to the movies'),\n",
        " Document(page_content='This is just a random text.'),\n",
        " Document(page_content='Fly me to the moon is one of my favourite songs.'),\n",
        " Document(page_content='Basquetball is a great sport.'),\n",
        " Document(page_content='Larry Bird was an iconic NBA player.'),\n",
        " Document(page_content='L. Kornet is one of the best Celtics players.'),\n",
        " Document(page_content='This is a document about the Boston Celtics')]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo-clJ6UtPHo"
      },
      "outputs": [],
      "source": [
        "# We prepare and run a custom Stuff chain with reordered docs as context.\n",
        "\n",
        "# Override prompts\n",
        "document_prompt = PromptTemplate(\n",
        "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
        ")\n",
        "document_variable_name = \"context\"\n",
        "llm = OpenAI()\n",
        "stuff_prompt_override = \"\"\"Given this text extracts:\n",
        "-----\n",
        "{context}\n",
        "-----\n",
        "Please answer the following question:\n",
        "{query}\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=stuff_prompt_override, input_variables=[\"context\", \"query\"]\n",
        ")\n",
        "\n",
        "# Instantiate the chain\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain,\n",
        "    document_prompt=document_prompt,\n",
        "    document_variable_name=document_variable_name,\n",
        ")\n",
        "chain.run(input_documents=reordered_docs, query=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cnwKJ0qFqef"
      },
      "source": [
        "# MultiVector Retriever\n",
        "\n",
        "It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base MultiVectorRetriever which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the MultiVectorRetriever.\n",
        "\n",
        "The methods to create multiple vectors per document include:\n",
        "\n",
        "- Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever).\n",
        "- Summary: create a summary for each document, embed that along with (or instead of) the document.\n",
        "- Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.\n",
        "Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control.\n",
        "\n",
        "문서당 여러 개의 벡터를 저장하는 것이 종종 유용할 수 있습니다. 이것이 유용한 여러 사용 사례가 있습니다. LangChain에는 이러한 유형의 설정을 쉽게 쿼리할 수 있는 기본 멀티벡터리트리버가 있습니다. 많은 복잡성은 문서당 여러 개의 벡터를 만드는 방법에 달려 있습니다. 이 노트북은 이러한 벡터를 만들고 멀티벡터리트리버를 사용하는 일반적인 방법 중 일부를 다룹니다.\n",
        "\n",
        "문서당 여러 개의 벡터를 만드는 방법은 다음과 같습니다:\n",
        "\n",
        "- 더 작은 청크: 문서를 더 작은 청크로 나누고 이 청크를 포함합니다(이 청크는 상위 문서 검색기입니다).\n",
        "- 요약: 각 문서에 대한 요약을 작성하여 문서와 함께 포함합니다(또는 대신).\n",
        "- 가상 질문: 각 문서가 답변하기에 적합한 가상 질문을 작성하여 문서와 함께(또는 대신) 포함합니다.\n",
        "이를 통해 수동으로 임베딩을 추가하는 다른 방법도 가능합니다. 문서를 복구하는 데 필요한 질문이나 쿼리를 명시적으로 추가할 수 있으므로 제어력이 향상됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvjCxLHUGAtZ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "\n",
        "from langchain.storage import InMemoryByteStore\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "loaders = [\n",
        "    TextLoader(\"../../paul_graham_essay.txt\"),\n",
        "    TextLoader(\"../../state_of_the_union.txt\"),\n",
        "]\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
        "docs = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWybK_0GGTBv"
      },
      "source": [
        "## Smaller chunks\n",
        "\n",
        "Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks. This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the ParentDocumentRetriever does. Here we show what is going on under the hood.\n",
        "\n",
        "종종 더 큰 정보 청크를 검색하지만 더 작은 청크를 임베딩하는 것이 유용할 수 있습니다. 이를 통해 임베딩은 가능한 한 의미론적 의미를 가깝게 포착할 수 있지만 가능한 한 많은 컨텍스트를 다운스트림으로 전달할 수 있습니다. 이것이 상위 문서 검색기가 수행하는 작업이라는 점에 주목하십시오. 여기서 우리는 후드 아래에서 무슨 일이 일어나고 있는지 보여줍니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HKjdjWIGf0H"
      },
      "outputs": [],
      "source": [
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "import uuid\n",
        "\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FmK-S0WGhWt"
      },
      "outputs": [],
      "source": [
        "# The splitter to use to create smaller chunks\n",
        "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dHyyGWPGheS"
      },
      "outputs": [],
      "source": [
        "sub_docs = []\n",
        "for i, doc in enumerate(docs):\n",
        "    _id = doc_ids[i]\n",
        "    _sub_docs = child_text_splitter.split_documents([doc])\n",
        "    for _doc in _sub_docs:\n",
        "        _doc.metadata[id_key] = _id\n",
        "    sub_docs.extend(_sub_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2xjko99Ghn2"
      },
      "outputs": [],
      "source": [
        "retriever.vectorstore.add_documents(sub_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhmooVqwGmhL"
      },
      "outputs": [],
      "source": [
        "# Vectorstore alone retrieves the small chunks\n",
        "retriever.vectorstore.similarity_search(\"justice breyer\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC_HveOQGgrT"
      },
      "source": [
        "The default search type the retriever performs on the vector database is a similarity search. LangChain Vector Stores also support searching via Max Marginal Relevance so if you want this instead you can just set the search_type property as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eByJRT8TGtRz"
      },
      "source": [
        "리트리버가 벡터 데이터베이스에서 수행하는 기본 검색 유형은 유사도 검색입니다. LangChain Vector Stores는 Max Marginal Religence를 통한 검색도 지원하므로 이를 원하는 경우 search_type 속성을 다음과 같이 설정할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qhDnEjpGuDL"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_vector import SearchType\n",
        "\n",
        "retriever.search_type = SearchType.mmr\n",
        "\n",
        "len(retriever.get_relevant_documents(\"justice breyer\")[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNWIlD7LG0mq"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.\n",
        "\n",
        "종종 요약은 청크가 무엇인지 더 정확하게 증류하여 더 나은 검색으로 이어질 수 있습니다. 여기서는 요약을 만드는 방법을 보여주고 그런 다음 이들을 임베딩하는 방법을 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvuPN2hNG0AS"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dMmM4owG708"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"doc\": lambda x: x.page_content}\n",
        "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
        "    | ChatOpenAI(max_retries=0)\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhzJquEYHEBd"
      },
      "outputs": [],
      "source": [
        "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tSG5TqCHE1e"
      },
      "outputs": [],
      "source": [
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQg6XDdnHNop"
      },
      "outputs": [],
      "source": [
        "summary_docs = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(summaries)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PmUFIAhHPav"
      },
      "outputs": [],
      "source": [
        "retriever.vectorstore.add_documents(summary_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERCr3atVHUDL"
      },
      "outputs": [],
      "source": [
        "# # We can also add the original chunks to the vectorstore if we so want\n",
        "# for i, doc in enumerate(docs):\n",
        "#     doc.metadata[id_key] = doc_ids[i]\n",
        "# retriever.vectorstore.add_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E09u_9O0HRVi"
      },
      "outputs": [],
      "source": [
        "sub_docs = vectorstore.similarity_search(\"justice breyer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evxadgzvHWwO"
      },
      "outputs": [],
      "source": [
        "sub_docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biT7oSJIHZ3k"
      },
      "outputs": [],
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(\"justice breyer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V9SyeewHcB6"
      },
      "outputs": [],
      "source": [
        "len(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tRturQFHepm"
      },
      "source": [
        "## Hypothetical Queries\n",
        "An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document. These questions can then be embedded\n",
        "\n",
        "또한 LLM을 사용하여 특정 문서에 대해 질문할 수 있는 가상 질문 목록을 생성할 수 있습니다. 그런 다음 이러한 질문을 포함할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmqqqJy2HoNk"
      },
      "outputs": [],
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"hypothetical_questions\",\n",
        "        \"description\": \"Generate hypothetical questions\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"questions\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\"type\": \"string\"},\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"questions\"],\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxjy_CaOHqIt"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
        "\n",
        "chain = (\n",
        "    {\"doc\": lambda x: x.page_content}\n",
        "    # Only asking for 3 hypothetical questions, but this could be adjusted\n",
        "    | ChatPromptTemplate.from_template(\n",
        "        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n",
        "    )\n",
        "    | ChatOpenAI(max_retries=0, model=\"gpt-4\").bind(\n",
        "        functions=functions, function_call={\"name\": \"hypothetical_questions\"}\n",
        "    )\n",
        "    | JsonKeyOutputFunctionsParser(key_name=\"questions\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juRtN_eBHrXd"
      },
      "outputs": [],
      "source": [
        "chain.invoke(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTYJBsGhHslg"
      },
      "outputs": [],
      "source": [
        "hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHAp0Q6HHul2"
      },
      "outputs": [],
      "source": [
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW1T3JrdHxdV"
      },
      "outputs": [],
      "source": [
        "question_docs = []\n",
        "for i, question_list in enumerate(hypothetical_questions):\n",
        "    question_docs.extend(\n",
        "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR4dZRJEHzDH"
      },
      "outputs": [],
      "source": [
        "retriever.vectorstore.add_documents(question_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gbc8jZNH0jH"
      },
      "outputs": [],
      "source": [
        "sub_docs = vectorstore.similarity_search(\"justice breyer\")\n",
        "sub_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQtYQpi2H30Q"
      },
      "outputs": [],
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(\"justice breyer\")\n",
        "\n",
        "len(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhLgED0qH6NE"
      },
      "source": [
        "# Parent Document Retriever\n",
        "\n",
        "When splitting documents for retrieval, there are often conflicting desires:\n",
        "\n",
        "1. You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
        "2. You want to have long enough documents that the context of each chunk is retained.\n",
        "The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n",
        "\n",
        "Note that “parent document” refers to the document that a small chunk originated from. This can either be the whole raw document OR a larger chunk.\n",
        "\n",
        "### 상위 문서 검색기\n",
        "\n",
        "검색을 위해 문서를 분할할 때 종종 상충되는 욕구가 있습니다:\n",
        "\n",
        "1. 임베딩이 의미를 가장 정확하게 반영할 수 있도록 작은 문서를 가지고 싶을 수도 있습니다. 너무 길면 임베딩이 의미를 잃을 수 있습니다.\n",
        "2. 각 청크의 컨텍스트가 유지될 만큼 긴 문서를 보유하려고 합니다.\n",
        "상위 문서 검색기는 작은 데이터 청크를 분할하여 저장함으로써 균형을 유지합니다. 검색하는 동안 먼저 작은 청크를 가져온 다음 해당 청크의 상위 ID를 찾아 더 큰 문서를 반환합니다.\n",
        "\n",
        "\"부모 문서\"는 작은 청크가 시작된 문서를 가리킵니다. 이것은 전체 원시 문서이거나 더 큰 청크일 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17kJHsVGIMWv"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKjWjFvHIQai"
      },
      "outputs": [],
      "source": [
        "from langchain.storage import InMemoryStore\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WonqYXfSIQhj"
      },
      "outputs": [],
      "source": [
        "loaders = [\n",
        "    TextLoader(\"../../paul_graham_essay.txt\"),\n",
        "    TextLoader(\"../../state_of_the_union.txt\"),\n",
        "]\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSZCLQ14IULK"
      },
      "source": [
        "## Retrieving full documents\n",
        "In this mode, we want to retrieve the full documents. Therefore, we only specify a child splitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvEgT4-ZITii"
      },
      "outputs": [],
      "source": [
        "# This text splitter is used to create the child documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")\n",
        "\n",
        "retriever.add_documents(docs, ids=None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCUgMX_vIgvI"
      },
      "source": [
        "This should yield two keys, because we added two documents.\n",
        "두 개의 문서를 추가했기 때문에 두 개의 키를 얻을 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Fk0hl3If4d"
      },
      "outputs": [],
      "source": [
        "list(store.yield_keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lletot4yIPag"
      },
      "source": [
        "```\n",
        "['cfdf4af7-51f2-4ea3-8166-5be208efa040',\n",
        " 'bf213c21-cc66-4208-8a72-733d030187e6']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhQ0Ghu3Ivnq"
      },
      "source": [
        "Let’s now call the vector store search functionality - we should see that it returns small chunks (since we’re storing the small chunks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3eYNTIIItda"
      },
      "outputs": [],
      "source": [
        "sub_docs = vectorstore.similarity_search(\"justice breyer\")\n",
        "\n",
        "print(sub_docs[0].page_content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-idKhd2I0jP"
      },
      "source": [
        "Let’s now retrieve from the overall retriever. This should return large documents - since it returns the documents where the smaller chunks are located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32HuuAbbI1Tn"
      },
      "outputs": [],
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(\"justice breyer\")\n",
        "len(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82mxialmI5hO"
      },
      "source": [
        "## Retrieving larger chunks\n",
        "\n",
        "Sometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents).\n",
        "\n",
        "전체 문서가 너무 커서 그대로 검색하고 싶어하지 않을 수도 있습니다. 이 경우, 우리가 정말로 원하는 것은 먼저 원시 문서를 더 큰 청크로 나눈 다음 더 작은 청크로 나누는 것입니다. 그런 다음 더 작은 청크를 인덱싱하지만 검색할 때 더 큰 청크를 검색합니다(그렇지만 전체 문서는 검색하지 않습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZyoDfYfJAxU"
      },
      "outputs": [],
      "source": [
        "# This text splitter is used to create the parent documents\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "# This text splitter is used to create the child documents\n",
        "# It should create documents smaller than the parent\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93oscu9eJFcx"
      },
      "outputs": [],
      "source": [
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBmITO3sJGxf"
      },
      "outputs": [],
      "source": [
        "retriever.add_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLy_kMIpJTho"
      },
      "source": [
        "We can see that there are much more than two documents now - these are the larger chunks.\n",
        "\n",
        "이제 문서가 두 개가 훨씬 넘는 것을 알 수 있습니다. 더 큰 덩어리들입니다.\n",
        "\n",
        "Let’s make sure the underlying vector store still retrieves the small chunks.\n",
        "\n",
        "기본 벡터 저장소가 여전히 작은 청크를 검색하는지 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtd0tLx4Jjcg"
      },
      "outputs": [],
      "source": [
        "sub_docs = vectorstore.similarity_search(\"justice breyer\")\n",
        "\n",
        "print(sub_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38kVyG_dJnf4"
      },
      "source": [
        "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\n",
        "\n",
        "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbkZY70kJome"
      },
      "outputs": [],
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(\"justice breyer\")\n",
        "\n",
        "len(retrieved_docs[0].page_content)\n",
        "\n",
        "print(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwtPDZ5GJzCa"
      },
      "source": [
        "# Self-querying\n",
        "\n",
        "Head to Integrations for documentation on vector stores with built-in support for self-querying.\n",
        "\n",
        "A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\n",
        "\n",
        "자체 쿼리 기능이 내장된 벡터 스토어에 대한 문서화를 위해 Integrations로 이동합니다.\n",
        "\n",
        "셀프 쿼리 리트리버는 이름에서 알 수 있듯이 스스로 쿼리할 수 있는 기능을 가지고 있습니다. 특히, 임의의 자연어 쿼리가 주어지면 리트리버는 쿼리 구성 LLM 체인을 사용하여 구조화된 쿼리를 작성한 다음 해당 구조화된 쿼리를 기본 VectorStore에 적용합니다. 이를 통해 리트리버는 사용자 입력 쿼리를 사용하여 저장된 문서의 내용과 의미론적 유사성을 비교할 수 있을 뿐만 아니라 저장된 문서의 메타데이터에 대한 사용자 쿼리에서 필터를 추출하고 해당 필터를 실행할 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFTBzlmHKFk2"
      },
      "source": [
        "## Get started\n",
        "\n",
        "For demonstration purposes we’ll use a Chroma vector store. We’ve created a small demo set of documents that contain summaries of movies.\n",
        "\n",
        "Note: The self-query retriever requires you to have lark package installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCPFYf89JvuK"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  lark chromadb\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
        "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
        "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
        "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
        "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Toys come alive and have a blast doing so\",\n",
        "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
        "        metadata={\n",
        "            \"year\": 1979,\n",
        "            \"director\": \"Andrei Tarkovsky\",\n",
        "            \"genre\": \"thriller\",\n",
        "            \"rating\": 9.9,\n",
        "        },\n",
        "    ),\n",
        "]\n",
        "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zltVnXtBKO17"
      },
      "source": [
        "## Creating our self-querying retriever\n",
        "\n",
        "Now we can instantiate our retriever. To do this we’ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ij3wCaxKN47"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "metadata_field_info = [\n",
        "    AttributeInfo(\n",
        "        name=\"genre\",\n",
        "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"year\",\n",
        "        description=\"The year the movie was released\",\n",
        "        type=\"integer\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"director\",\n",
        "        description=\"The name of the movie director\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
        "    ),\n",
        "]\n",
        "document_content_description = \"Brief summary of a movie\"\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "retriever = SelfQueryRetriever.from_llm(\n",
        "    llm,\n",
        "    vectorstore,\n",
        "    document_content_description,\n",
        "    metadata_field_info,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T5nJpqmKWkF"
      },
      "source": [
        "## Testing it out\n",
        "And now we can actually try using our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMLmQGjuJp85"
      },
      "outputs": [],
      "source": [
        "# This example only specifies a filter\n",
        "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8saS64TKcpZ"
      },
      "outputs": [],
      "source": [
        "# This example specifies a query and a filter\n",
        "retriever.invoke(\"Has Greta Gerwig directed any movies about women\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeiDK1UVKfDD"
      },
      "outputs": [],
      "source": [
        "# This example specifies a composite filter\n",
        "retriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdG6liZUKhNK"
      },
      "outputs": [],
      "source": [
        "# This example specifies a query and composite filter\n",
        "retriever.invoke(\n",
        "    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB9NZ7EpKkJn"
      },
      "source": [
        "## Filter k\n",
        "We can also use the self query retriever to specify k: the number of documents to fetch.\n",
        "\n",
        "We can do this by passing enable_limit=True to the constructor.\n",
        "\n",
        "또한 자체 쿼리 검색기를 사용하여 k: 가져올 문서 수를 지정할 수 있습니다.\n",
        "\n",
        "enable_limit= True를 생성자에게 전달하면 이 작업을 수행할 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiV7w7NPKtMH"
      },
      "outputs": [],
      "source": [
        "retriever = SelfQueryRetriever.from_llm(\n",
        "    llm,\n",
        "    vectorstore,\n",
        "    document_content_description,\n",
        "    metadata_field_info,\n",
        "    enable_limit=True,\n",
        ")\n",
        "\n",
        "# This example only specifies a relevant query\n",
        "retriever.invoke(\"What are two movies about dinosaurs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQhjd-wNKv3E"
      },
      "source": [
        "## Constructing from scratch with LCEL\n",
        "To see what’s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.\n",
        "\n",
        "First, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we’ll ignore here for simplicity.\n",
        "\n",
        "후드 아래에서 무슨 일이 일어나고 있는지 확인하고 더 많은 사용자 정의 제어를 위해 처음부터 리트리버를 재구성할 수 있습니다.\n",
        "\n",
        "먼저 쿼리-구축 체인을 만들어야 합니다. 이 체인은 사용자 쿼리를 수행하고 사용자가 지정한 필터를 캡처하는 StructuredQuery 개체를 생성합니다. 저희는 프롬프트 및 출력 파서를 만들기 위한 몇 가지 도우미 기능을 제공합니다. 여기서는 단순화를 위해 무시할 조정 가능한 여러 개의 파라미터를 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsmurWSwKuLX"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.query_constructor.base import (\n",
        "    StructuredQueryOutputParser,\n",
        "    get_query_constructor_prompt,\n",
        ")\n",
        "\n",
        "prompt = get_query_constructor_prompt(\n",
        "    document_content_description,\n",
        "    metadata_field_info,\n",
        ")\n",
        "output_parser = StructuredQueryOutputParser.from_components()\n",
        "query_constructor = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjNKZwrJK8xe"
      },
      "outputs": [],
      "source": [
        "print(prompt.format(query=\"dummy question\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgpzb4OKLBYb"
      },
      "source": [
        "```\n",
        "  Your goal is to structure the user's query to match the request schema provided below.\n",
        "\n",
        "  << Structured Request Schema >>\n",
        "  When responding use a markdown code snippet with a JSON object formatted in the following schema:\n",
        "\n",
        "  json\n",
        "  {\n",
        "      \"query\": string \\ text string to compare to document contents\n",
        "      \"filter\": string \\ logical condition statement for filtering documents\n",
        "  }\n",
        "  \n",
        "\n",
        "  The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\n",
        "\n",
        "  A logical condition statement is composed of one or more comparison and logical operation statements.\n",
        "\n",
        "  A comparison statement takes the form: `comp(attr, val)`:\n",
        "  - `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator\n",
        "  - `attr` (string):  name of attribute to apply the comparison to\n",
        "  - `val` (string): is the comparison value\n",
        "\n",
        "  A logical operation statement takes the form `op(statement1, statement2, ...)`:\n",
        "  - `op` (and | or | not): logical operator\n",
        "  - `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\n",
        "\n",
        "  Make sure that you only use the comparators and logical operators listed above and no others.\n",
        "  Make sure that filters only refer to attributes that exist in the data source.\n",
        "  Make sure that filters only use the attributed names with its function names if there are functions applied on them.\n",
        "  Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\n",
        "  Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\n",
        "  Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.\n",
        "\n",
        "  << Example 1. >>\n",
        "  Data Source:\n",
        "  json\n",
        "  {\n",
        "      \"content\": \"Lyrics of a song\",\n",
        "      \"attributes\": {\n",
        "          \"artist\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"Name of the song artist\"\n",
        "          },\n",
        "          \"length\": {\n",
        "              \"type\": \"integer\",\n",
        "              \"description\": \"Length of the song in seconds\"\n",
        "          },\n",
        "          \"genre\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "\n",
        "\n",
        "  User Query:\n",
        "  What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre\n",
        "\n",
        "  Structured Request:\n",
        "  json\n",
        "  {\n",
        "      \"query\": \"teenager love\",\n",
        "      \"filter\": \"and(or(eq(\\\"artist\\\", \\\"Taylor Swift\\\"), eq(\\\"artist\\\", \\\"Katy Perry\\\")), lt(\\\"length\\\", 180), eq(\\\"genre\\\", \\\"pop\\\"))\"\n",
        "  }\n",
        "  \n",
        "\n",
        "\n",
        "  << Example 2. >>\n",
        "  Data Source:\n",
        "  json\n",
        "  {\n",
        "      \"content\": \"Lyrics of a song\",\n",
        "      \"attributes\": {\n",
        "          \"artist\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"Name of the song artist\"\n",
        "          },\n",
        "          \"length\": {\n",
        "              \"type\": \"integer\",\n",
        "              \"description\": \"Length of the song in seconds\"\n",
        "          },\n",
        "          \"genre\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "  \n",
        "\n",
        "  User Query:\n",
        "  What are songs that were not published on Spotify\n",
        "\n",
        "  Structured Request:\n",
        "  json\n",
        "  {\n",
        "      \"query\": \"\",\n",
        "      \"filter\": \"NO_FILTER\"\n",
        "  }\n",
        "  \n",
        "\n",
        "\n",
        "  << Example 3. >>\n",
        "  Data Source:\n",
        "  json\n",
        "  {\n",
        "      \"content\": \"Brief summary of a movie\",\n",
        "      \"attributes\": {\n",
        "      \"genre\": {\n",
        "          \"description\": \"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
        "          \"type\": \"string\"\n",
        "      },\n",
        "      \"year\": {\n",
        "          \"description\": \"The year the movie was released\",\n",
        "          \"type\": \"integer\"\n",
        "      },\n",
        "      \"director\": {\n",
        "          \"description\": \"The name of the movie director\",\n",
        "          \"type\": \"string\"\n",
        "      },\n",
        "      \"rating\": {\n",
        "          \"description\": \"A 1-10 rating for the movie\",\n",
        "          \"type\": \"float\"\n",
        "      }\n",
        "  }\n",
        "  }\n",
        "  \n",
        "\n",
        "  User Query:\n",
        "  dummy question\n",
        "\n",
        "  Structured Request:\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSBirsXQLVrF"
      },
      "source": [
        "The query constructor is the key element of the self-query retriever. To make a great retrieval system you’ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.\n",
        "\n",
        "The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you’re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.\n",
        "\n",
        "쿼리 컨스트럭터는 셀프 쿼리 리트리버의 핵심 요소입니다. 훌륭한 검색 시스템을 만들려면 쿼리 컨스트럭터가 제대로 작동하는지 확인해야 합니다. 종종 프롬프트, 프롬프트의 예제, 속성 설명 등을 조정해야 합니다. 일부 호텔 인벤토리 데이터에 대한 쿼리 컨스트럭터를 개선하는 예를 보려면 이 요리책을 확인하십시오.\n",
        "\n",
        "다음 핵심 요소는 Structured Query Translator입니다. 사용 중인 벡터 스토어의 구문에서 일반 StructuredQuery 개체를 메타데이터 필터로 변환하는 역할을 하는 개체입니다. LangChain에는 여러 개의 기본 제공되는 번역기가 있습니다. 이들을 모두 보려면 Integrations 섹션으로 이동합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9McAoLRKLarK"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.self_query.chroma import ChromaTranslator\n",
        "\n",
        "retriever = SelfQueryRetriever(\n",
        "    query_constructor=query_constructor,\n",
        "    vectorstore=vectorstore,\n",
        "    structured_query_translator=ChromaTranslator(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly_biZiULbc5"
      },
      "outputs": [],
      "source": [
        "retriever.invoke(\n",
        "    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQJ2EEiLLhgr"
      },
      "source": [
        "# Time-weighted vector store retriever\n",
        "This retriever uses a combination of semantic similarity and a time decay.\n",
        "\n",
        "The algorithm for scoring them is:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wENR0k_QLlH4"
      },
      "outputs": [],
      "source": [
        "semantic_similarity + (1.0 - decay_rate) ^ hours_passed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G99yJBcLpSI"
      },
      "source": [
        "Notably, hours_passed refers to the hours passed since the object in the retriever was last accessed, not since it was created. This means that frequently accessed objects remain “fresh”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu-DfSXjLoGB"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "import faiss\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBaYV9tXLszT"
      },
      "source": [
        "## Low decay rate\n",
        "A low decay rate (in this, to be extreme, we will set it close to 0) means memories will be “remembered” for longer. A decay rate of 0 means memories never be forgotten, making this retriever equivalent to the vector lookup.\n",
        "\n",
        "낮은 감쇠율(극단적으로 0에 가깝게 설정합니다)은 기억이 더 오래 \"기억\"된다는 것을 의미합니다. 감쇠율이 0이라는 것은 기억이 결코 잊혀지지 않는다는 것을 의미하며, 이 리트리버는 벡터 룩업과 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTKW59HOL1BU"
      },
      "outputs": [],
      "source": [
        "# Define your embedding model\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "# Initialize the vectorstore as empty\n",
        "embedding_size = 1536\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n",
        "retriever = TimeWeightedVectorStoreRetriever(\n",
        "    vectorstore=vectorstore, decay_rate=0.0000000000000000000000001, k=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5eaXIT_L2Qg"
      },
      "outputs": [],
      "source": [
        "yesterday = datetime.now() - timedelta(days=1)\n",
        "retriever.add_documents(\n",
        "    [Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday})]\n",
        ")\n",
        "retriever.add_documents([Document(page_content=\"hello foo\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUjxp4jmL5RV"
      },
      "outputs": [],
      "source": [
        "# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough\n",
        "retriever.get_relevant_documents(\"hello world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIPKX3hzL69A"
      },
      "source": [
        "## High decay rate\n",
        "With a high decay rate (e.g., several 9’s), the recency score quickly goes to 0! If you set this all the way to 1, recency is 0 for all objects, once again making this equivalent to a vector lookup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwz4B6_AMAbP"
      },
      "outputs": [],
      "source": [
        "# Define your embedding model\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "# Initialize the vectorstore as empty\n",
        "embedding_size = 1536\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n",
        "retriever = TimeWeightedVectorStoreRetriever(\n",
        "    vectorstore=vectorstore, decay_rate=0.999, k=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvMyrwOWMBIm"
      },
      "outputs": [],
      "source": [
        "yesterday = datetime.now() - timedelta(days=1)\n",
        "retriever.add_documents(\n",
        "    [Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday})]\n",
        ")\n",
        "retriever.add_documents([Document(page_content=\"hello foo\")])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NczY6FI0MDAI"
      },
      "outputs": [],
      "source": [
        "# \"Hello Foo\" is returned first because \"hello world\" is mostly forgotten\n",
        "retriever.get_relevant_documents(\"hello world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl6cH3T6MFOr"
      },
      "source": [
        "## Virtual time\n",
        "Using some utils in LangChain, you can mock out the time component.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29O59GkmMKhV"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "from langchain.utils import mock_now\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTMqndthMMCe"
      },
      "outputs": [],
      "source": [
        "# Notice the last access time is that date time\n",
        "with mock_now(datetime.datetime(2024, 2, 3, 10, 11)):\n",
        "    print(retriever.get_relevant_documents(\"hello world\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW5d_SoaMJ11"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
